from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import pandas as pd
# Generate a random dataset for classification
X, y = make_classification(n_samples=1000, n_features=10, random_state=0)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a Random Forest classifier
rf = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the classifier on the training data
rf.fit(X_train, y_train)
# Generate a random dataset for classification
X, y = make_classification(n_samples=1000, n_features=10, random_state=42)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


# Generate a random dataset
X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=0, random_state=42)

# Create a Random Forest classifier
rf = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model
rf.fit(X, y)

# Print the feature importance scores
importances = rf.feature_importances_
for feature, importance in zip(range(X.shape[1]), importances):
    print('Feature %d: %f' % (feature, importance))



=================================================================================================
1. Prefermance evaluation

Compute the accuracy score, confusion matrix, precision and recall using the testing dataset.

Then select the correct answers


The accuracy score is 0.75


The accuracy score is higher than 0.70


The precision is 0.89


The precision is 0.948 and the recall is 0.958

Correct!

4
True or False: Random Forest models generate non-linear decision boundaries.


False


True

5
True or False: Random Forest models are highly interpretable due to the simplicity of the decision trees that they are composed of.


True


False

